{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwAjGIyT5_7v",
        "outputId": "6e6fe889-a543-4c96-d7cf-0e2caadea50a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1NI1-gM6A83"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.metrics import Mean\n",
        "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Lambda, Concatenate\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from skimage.segmentation import quickshift\n",
        "from skimage.filters import threshold_otsu\n",
        "import cv2\n",
        "import pickle\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Lambda\n",
        "import functools\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras.utils import Sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yb3EE8jA6LNy"
      },
      "outputs": [],
      "source": [
        "# 0. define top 1, ...\n",
        "\n",
        "top2_acc = functools.partial(metrics.top_k_categorical_accuracy, k=2)\n",
        "top3_acc = functools.partial(metrics.top_k_categorical_accuracy, k=3)\n",
        "top4_acc = functools.partial(metrics.top_k_categorical_accuracy, k=4)\n",
        "top5_acc = functools.partial(metrics.top_k_categorical_accuracy, k=5)\n",
        "top2_acc.__name__ = 'top2_acc'\n",
        "top3_acc.__name__ = 'top3_acc'\n",
        "top4_acc.__name__ = 'top4_acc'\n",
        "top5_acc.__name__ = 'top5_acc'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBPvfIRv6VDx"
      },
      "outputs": [],
      "source": [
        "# 1. Define helper functions\n",
        "\n",
        "def extract_thermal_face(image):\n",
        "    image_float = image.astype(float) / 255 if image.max() > 1 else image\n",
        "    segments = quickshift(image_float, ratio=1.0, kernel_size=3, max_dist=6)\n",
        "    threshold = threshold_otsu(segments)\n",
        "    binary = segments > threshold\n",
        "    face_mask = binary.astype(np.uint8) * 255\n",
        "    face_region = cv2.bitwise_and(image, image, mask=face_mask)\n",
        "    return face_region\n",
        "\n",
        "def load_and_preprocess_image(image_path):\n",
        "    img = load_img(image_path, target_size=(72, 96))\n",
        "    img_array = img_to_array(img)\n",
        "    thermal_face = extract_thermal_face(img_array)\n",
        "    thermal_face = np.expand_dims(thermal_face, axis=0) / 255.0\n",
        "    return thermal_face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23UfLxrR6dsl"
      },
      "outputs": [],
      "source": [
        "# 2. define loss function\n",
        "\n",
        "centers = tf.Variable(tf.zeros([16, 128]), trainable=False)\n",
        "\n",
        "def contrastive_loss(y_true, y_pred, margin=1):\n",
        "    y_true = tf.cast(y_true, 'float32')\n",
        "    square_pred = tf.square(y_pred)\n",
        "    margin_square = tf.square(tf.maximum(margin - y_pred, 0))\n",
        "    return tf.reduce_mean(y_true * square_pred + (1 - y_true) * margin_square)\n",
        "\n",
        "def center_loss(y_true, y_pred):\n",
        "    alpha = 0.5\n",
        "    y_true = tf.cast(y_true, 'int32')\n",
        "    y_true_matrix = tf.one_hot(y_true, depth=16)\n",
        "    centers_batch = tf.gather(centers, y_true)\n",
        "    diff = centers_batch - y_pred\n",
        "    unique_labels, unique_idx, unique_counts = tf.unique_with_counts(y_true)\n",
        "    appear_times = tf.gather(unique_counts, unique_idx)\n",
        "    appear_times = tf.reshape(appear_times, (-1, 1))\n",
        "    diff /= tf.cast((1 + appear_times), tf.float32)\n",
        "    diff *= alpha\n",
        "    centers_update = tf.tensor_scatter_nd_sub(centers, tf.reshape(y_true, [-1, 1]), diff)\n",
        "    with tf.control_dependencies([centers_update]):\n",
        "        centers_batch_updated = tf.gather(centers, y_true)\n",
        "    loss = tf.reduce_mean(tf.reduce_sum(tf.square(y_pred - centers_batch_updated), axis=1))\n",
        "    return loss\n",
        "\n",
        "def combined_loss(y_true, y_pred):\n",
        "    # Ensure y_true has the expected shape\n",
        "    if tf.shape(y_true)[1] != 3:\n",
        "        tf.print(\"Error: y_true does not have the expected shape.\")\n",
        "        return 0.0\n",
        "\n",
        "    # Ensure y_pred has the expected shape (batch_size, 257)\n",
        "    if tf.shape(y_pred)[1] != 257:\n",
        "        tf.print(\"Error: y_pred does not have the expected shape.\")\n",
        "        return 0.0\n",
        "\n",
        "    # Unpack y_pred\n",
        "    distance = y_pred[:, 0]\n",
        "    embedding_1 = y_pred[:, 1:129]\n",
        "    embedding_2 = y_pred[:, 129:]\n",
        "\n",
        "    # Calculate losses\n",
        "    cont_loss = contrastive_loss(y_true[:, 0], distance)\n",
        "    cent_loss_1 = center_loss(tf.cast(y_true[:, 1], tf.int32), embedding_1)\n",
        "    cent_loss_2 = center_loss(tf.cast(y_true[:, 2], tf.int32), embedding_2)\n",
        "\n",
        "    total_loss = cont_loss + 0.1 * (cent_loss_1 + cent_loss_2)\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c71aGTGz6kr9"
      },
      "outputs": [],
      "source": [
        "# 3. create embeddings\n",
        "\n",
        "base_model = load_model('/content/drive/MyDrive/WORKS/FACIAL RECOGNITION RESEARCH]/data/finetuned_thermal_face_vgg16model.h5', custom_objects={\n",
        "    'top2_acc': top2_acc,\n",
        "    'top3_acc': top3_acc,\n",
        "    'top4_acc': top4_acc,\n",
        "    'top5_acc': top5_acc\n",
        "})\n",
        "\n",
        "x = base_model.get_layer('batch_normalization_1').output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "embedding = Dense(128, name='embedding')(x)\n",
        "\n",
        "embedding_model = Model(inputs=base_model.input, outputs=embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oW4hZtl69CS",
        "outputId": "c4866419-47eb-4ad0-cb77-6cbb76104dcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model (None, 2, 3, 512)\n",
            "max_pooling2d_1 (None, 1, 1, 512)\n",
            "batch_normalization_1 (None, 1, 1, 512)\n",
            "flatten_1 (None, 512)\n",
            "dense_1 (None, 16)\n"
          ]
        }
      ],
      "source": [
        "# Print output shapes of all layers\n",
        "for layer in base_model.layers:\n",
        "    print(layer.name, layer.output_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFGJt1-C6-gE"
      },
      "outputs": [],
      "source": [
        "# 4. Create the Siamese network for contrastive loss\n",
        "\n",
        "input_1 = Input(shape=(72, 96, 3))\n",
        "input_2 = Input(shape=(72, 96, 3))\n",
        "\n",
        "embedding_1 = embedding_model(input_1)\n",
        "embedding_2 = embedding_model(input_2)\n",
        "\n",
        "distance = Lambda(lambda x: K.sqrt(K.sum(K.square(x[0] - x[1]), axis=1, keepdims=True)))([embedding_1, embedding_2])\n",
        "\n",
        "siamese_model = Model(inputs=[input_1, input_2], outputs=[distance, embedding_1, embedding_2])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyQu29TEy6Ii"
      },
      "outputs": [],
      "source": [
        "class ContrastiveAccuracy(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='contrastive_accuracy', **kwargs):\n",
        "        super(ContrastiveAccuracy, self).__init__(name=name, **kwargs)\n",
        "        self.correct_counter = self.add_weight(name='correct', initializer='zeros')\n",
        "        self.total_counter = self.add_weight(name='total', initializer='zeros')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        # Assuming y_pred[:, 0] is the distance\n",
        "        predictions = tf.cast(y_pred[:, 0] < 0.5, tf.float32)\n",
        "        values = tf.cast(tf.equal(predictions, y_true[:, 0]), tf.float32)\n",
        "        self.correct_counter.assign_add(tf.reduce_sum(values))\n",
        "        self.total_counter.assign_add(tf.cast(tf.size(values), tf.float32))\n",
        "\n",
        "    def result(self):\n",
        "        return self.correct_counter / self.total_counter\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.correct_counter.assign(0)\n",
        "        self.total_counter.assign(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_wyWs7G0JxA"
      },
      "outputs": [],
      "source": [
        "class Rank1Accuracy(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='rank1_accuracy', **kwargs):\n",
        "        super(Rank1Accuracy, self).__init__(name=name, **kwargs)\n",
        "        self.total_correct = self.add_weight(name='total_correct', initializer='zeros')\n",
        "        self.total = self.add_weight(name='total', initializer='zeros')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        margin = 1  # or another value depending on how you define rank 1 accuracy\n",
        "        distances = y_pred[:, 0]\n",
        "        predictions = tf.cast(distances < margin, dtype=tf.float32)\n",
        "        correct = tf.cast(tf.equal(predictions, y_true[:, 0]), dtype=tf.float32)\n",
        "        self.total_correct.assign_add(tf.reduce_sum(correct))\n",
        "        self.total.assign_add(tf.cast(tf.size(correct), tf.float32))  # Ensure float32 casting\n",
        "\n",
        "    def result(self):\n",
        "        return self.total_correct / self.total\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.total_correct.assign(0)\n",
        "        self.total.assign(0)\n",
        "class FalseAcceptanceRate(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='false_acceptance_rate', **kwargs):\n",
        "        super(FalseAcceptanceRate, self).__init__(name=name, **kwargs)\n",
        "        self.false_acceptances = self.add_weight(name='false_acceptances', initializer='zeros')\n",
        "        self.total_negative = self.add_weight(name='total_negative', initializer='zeros')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        threshold = 0.5  # Adjust threshold as necessary\n",
        "        predictions = tf.cast(y_pred[:, 0] < threshold, dtype=tf.float32)\n",
        "        false_acceptances = tf.logical_and(predictions == 1, y_true[:, 0] == 0)\n",
        "        self.false_acceptances.assign_add(tf.reduce_sum(tf.cast(false_acceptances, dtype=tf.float32)))\n",
        "        self.total_negative.assign_add(tf.cast(tf.reduce_sum(tf.cast(y_true[:, 0] == 0, dtype=tf.float32)), tf.float32))\n",
        "\n",
        "    def result(self):\n",
        "        return self.false_acceptances / self.total_negative\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.false_acceptances.assign(0)\n",
        "        self.total_negative.assign(0)\n",
        "\n",
        "class FalseRejectionRate(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='false_rejection_rate', **kwargs):\n",
        "        super(FalseRejectionRate, self).__init__(name=name, **kwargs)\n",
        "        self.false_rejections = self.add_weight(name='false_rejections', initializer='zeros')\n",
        "        self.total_positive = self.add_weight(name='total_positive', initializer='zeros')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        threshold = 0.5  # Adjust threshold as necessary\n",
        "        predictions = tf.cast(y_pred[:, 0] >= threshold, dtype=tf.float32)\n",
        "        false_rejections = tf.logical_and(predictions == 1, y_true[:, 0] == 1)\n",
        "        self.false_rejections.assign_add(tf.reduce_sum(tf.cast(false_rejections, dtype=tf.float32)))\n",
        "        self.total_positive.assign_add(tf.cast(tf.reduce_sum(tf.cast(y_true[:, 0] == 1, dtype=tf.float32)), tf.float32))\n",
        "\n",
        "    def result(self):\n",
        "        return self.false_rejections / self.total_positive\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.false_rejections.assign(0)\n",
        "        self.total_positive.assign(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-kwo23z7Q4Q"
      },
      "outputs": [],
      "source": [
        "# Siamese Model with custom metrics\n",
        "class SiameseModel(tf.keras.Model):\n",
        "    def __init__(self, siamese_network, **kwargs):\n",
        "        super(SiameseModel, self).__init__(**kwargs)\n",
        "        self.siamese_network = siamese_network\n",
        "        self.contrastive_accuracy = ContrastiveAccuracy()\n",
        "        self.rank1_accuracy = Rank1Accuracy()\n",
        "        self.false_acceptance_rate = FalseAcceptanceRate()\n",
        "        self.false_rejection_rate = FalseRejectionRate()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.siamese_network(inputs)\n",
        "\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)\n",
        "            loss = self.compiled_loss(y, y_pred)\n",
        "\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "        self.contrastive_accuracy.update_state(y, y_pred)\n",
        "        self.rank1_accuracy.update_state(y, y_pred)\n",
        "        self.false_acceptance_rate.update_state(y, y_pred)\n",
        "        self.false_rejection_rate.update_state(y, y_pred)\n",
        "\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        x, y = data\n",
        "        y_pred = self(x, training=False)\n",
        "        loss = self.compiled_loss(y, y_pred)\n",
        "\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "        self.contrastive_accuracy.update_state(y, y_pred)\n",
        "        self.rank1_accuracy.update_state(y, y_pred)\n",
        "        self.false_acceptance_rate.update_state(y, y_pred)\n",
        "        self.false_rejection_rate.update_state(y, y_pred)\n",
        "\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(SiameseModel, self).get_config()\n",
        "        config.update({\n",
        "            'siamese_network': tf.keras.utils.serialize_keras_object(self.siamese_network)\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        siamese_network = tf.keras.utils.deserialize_keras_object(config['siamese_network'])\n",
        "        return cls(siamese_network=siamese_network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6NbjT8tvRqw"
      },
      "outputs": [],
      "source": [
        "def create_siamese_network(input_shape):\n",
        "    input_1 = Input(shape=input_shape)\n",
        "    input_2 = Input(shape=input_shape)\n",
        "\n",
        "    # Load base model with custom accuracy functions if required\n",
        "    base_model = load_model('/content/drive/MyDrive/WORKS/FACIAL RECOGNITION RESEARCH]/data/finetuned_thermal_face_vgg16model.h5', custom_objects={\n",
        "        'top2_acc': top2_acc,\n",
        "        'top3_acc': top3_acc,\n",
        "        'top4_acc': top4_acc,\n",
        "        'top5_acc': top5_acc\n",
        "    })\n",
        "\n",
        "    x = base_model.get_layer('batch_normalization_1').output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    embedding = Dense(128, name='embedding')(x)\n",
        "\n",
        "    embedding_model = Model(inputs=base_model.input, outputs=embedding)\n",
        "\n",
        "    embedding_1 = embedding_model(input_1)\n",
        "    embedding_2 = embedding_model(input_2)\n",
        "\n",
        "    distance = Lambda(lambda tensors: tf.sqrt(tf.reduce_sum(tf.square(tensors[0] - tensors[1]), axis=1, keepdims=True)))([embedding_1, embedding_2])\n",
        "    merged_output = Concatenate()([distance, embedding_1, embedding_2])\n",
        "\n",
        "    return Model(inputs=[input_1, input_2], outputs=merged_output)\n",
        "\n",
        "siamese_network = create_siamese_network((72, 96, 3))\n",
        "siamese_model = SiameseModel(siamese_network)\n",
        "\n",
        "dummy_input = [tf.zeros((1, 72, 96, 3)), tf.zeros((1, 72, 96, 3))]\n",
        "_ = siamese_model(dummy_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHlZrf6AwVNR"
      },
      "outputs": [],
      "source": [
        "siamese_model.compile(\n",
        "    optimizer=Adam(learning_rate=0.0001),\n",
        "    loss=combined_loss,\n",
        "    metrics=[\n",
        "        siamese_model.contrastive_accuracy,\n",
        "        siamese_model.rank1_accuracy,\n",
        "        siamese_model.false_acceptance_rate,\n",
        "        siamese_model.false_rejection_rate\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJJSbVA4Aplf",
        "outputId": "d272dcf4-2bbb-4d60-e384-9e990e6360b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"siamese_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " model_3 (Functional)        (None, 257)               14880960  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14880968 (56.77 MB)\n",
            "Trainable params: 14879936 (56.76 MB)\n",
            "Non-trainable params: 1032 (4.03 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "siamese_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yA6w07Y57izM"
      },
      "outputs": [],
      "source": [
        "class SiameseGenerator(Sequence):\n",
        "    def __init__(self, image_paths, labels, batch_size=32, dim=(72, 96), n_channels=3):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.dim = dim\n",
        "        self.n_channels = n_channels\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.image_paths) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        list_IDs_temp = [self.image_paths[k] for k in indexes]\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indexes = np.arange(len(self.image_paths))\n",
        "        np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        X1 = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
        "        X2 = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
        "        y = np.empty((self.batch_size, 3), dtype=float)\n",
        "\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            X1[i,] = self.load_image(ID)\n",
        "\n",
        "            # Randomly choose another image\n",
        "            other_ID = np.random.choice(self.image_paths)\n",
        "            X2[i,] = self.load_image(other_ID)\n",
        "\n",
        "            # Set binary label: 1 if same class, 0 if different\n",
        "            same_class = self.labels[ID]['class_label'] == self.labels[other_ID]['class_label']\n",
        "            y[i,] = [float(same_class), float(self.labels[ID]['class_label']), float(self.labels[other_ID]['class_label'])]\n",
        "\n",
        "        return [X1, X2], y\n",
        "\n",
        "    def load_image(self, image_path):\n",
        "        img = tf.keras.preprocessing.image.load_img(image_path, target_size=self.dim)\n",
        "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
        "        return img / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDyH53e-7rNw"
      },
      "outputs": [],
      "source": [
        "# 7. Train the model\n",
        "def prepare_paths_and_labels(base_dir, n_classes=16):\n",
        "    paths = []\n",
        "    labels = {}\n",
        "\n",
        "    for class_id in range(1, n_classes + 1):\n",
        "        class_dir = os.path.join(base_dir, f'face{class_id}')\n",
        "        if not os.path.exists(class_dir):\n",
        "            continue\n",
        "        for img_name in os.listdir(class_dir):\n",
        "            img_path = os.path.join(class_dir, img_name)\n",
        "            if img_name.endswith('.jpg') or img_name.endswith('.png'):\n",
        "                paths.append(img_path)\n",
        "                labels[img_path] = {\n",
        "                    \"binary_label\": 0,  # You need to define how to set the binary label\n",
        "                    \"class_label\": class_id - 1  # Assuming class labels are from 0 to n_classes-1\n",
        "                }\n",
        "    return paths, labels\n",
        "\n",
        "# Define base directories for training and validation\n",
        "train_base_dir = '/content/drive/MyDrive/WORKS/FACIAL RECOGNITION RESEARCH]/data/ExtractedTerravicDatabase_subset/train'\n",
        "val_base_dir = '/content/drive/MyDrive/WORKS/FACIAL RECOGNITION RESEARCH]/data/ExtractedTerravicDatabase_subset/val'\n",
        "\n",
        "# Prepare paths and labels for training and validation sets\n",
        "train_paths, train_labels = prepare_paths_and_labels(train_base_dir)\n",
        "val_paths, val_labels = prepare_paths_and_labels(val_base_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nhAfvJT-bvT",
        "outputId": "b0c25383-b2ef-4e23-effd-fd474c14c73c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "29/29 [==============================] - 1058s 36s/step - loss: 4.5774 - contrastive_accuracy: 0.8653 - rank1_accuracy: 0.8653 - false_acceptance_rate: 0.0000e+00 - false_rejection_rate: 1.0000 - val_loss: 17.8640 - val_contrastive_accuracy: 0.8396 - val_rank1_accuracy: 0.8604 - val_false_acceptance_rate: 0.0000e+00 - val_false_rejection_rate: 0.9390\n",
            "Epoch 2/10\n",
            "29/29 [==============================] - 17s 604ms/step - loss: 0.8351 - contrastive_accuracy: 0.8384 - rank1_accuracy: 0.8728 - false_acceptance_rate: 0.0000e+00 - false_rejection_rate: 1.0000 - val_loss: 5.3153 - val_contrastive_accuracy: 0.8750 - val_rank1_accuracy: 0.9312 - val_false_acceptance_rate: 0.0000e+00 - val_false_rejection_rate: 0.8824\n",
            "Epoch 3/10\n",
            "29/29 [==============================] - 11s 392ms/step - loss: 0.4174 - contrastive_accuracy: 0.8211 - rank1_accuracy: 0.8901 - false_acceptance_rate: 0.0000e+00 - false_rejection_rate: 0.9881 - val_loss: 1.4066 - val_contrastive_accuracy: 0.8896 - val_rank1_accuracy: 0.9812 - val_false_acceptance_rate: 0.0000e+00 - val_false_rejection_rate: 0.7681\n",
            "Epoch 4/10\n",
            "29/29 [==============================] - 11s 387ms/step - loss: 0.2395 - contrastive_accuracy: 0.8772 - rank1_accuracy: 0.6853 - false_acceptance_rate: 0.0000e+00 - false_rejection_rate: 0.9048 - val_loss: 0.5145 - val_contrastive_accuracy: 0.9521 - val_rank1_accuracy: 1.0000 - val_false_acceptance_rate: 0.0000e+00 - val_false_rejection_rate: 0.3898\n",
            "Epoch 5/10\n",
            "29/29 [==============================] - 11s 380ms/step - loss: 0.2017 - contrastive_accuracy: 0.8728 - rank1_accuracy: 0.5690 - false_acceptance_rate: 0.0000e+00 - false_rejection_rate: 0.9291 - val_loss: 0.2942 - val_contrastive_accuracy: 0.9438 - val_rank1_accuracy: 0.9875 - val_false_acceptance_rate: 0.0000e+00 - val_false_rejection_rate: 0.3913\n",
            "Epoch 6/10\n",
            "29/29 [==============================] - 11s 395ms/step - loss: 0.1600 - contrastive_accuracy: 0.8912 - rank1_accuracy: 0.4246 - false_acceptance_rate: 0.0000e+00 - false_rejection_rate: 0.8487 - val_loss: 0.1867 - val_contrastive_accuracy: 0.9646 - val_rank1_accuracy: 0.7417 - val_false_acceptance_rate: 0.0000e+00 - val_false_rejection_rate: 0.2537\n",
            "Epoch 7/10\n",
            "29/29 [==============================] - 11s 382ms/step - loss: 0.1680 - contrastive_accuracy: 0.8890 - rank1_accuracy: 0.4073 - false_acceptance_rate: 0.0000e+00 - false_rejection_rate: 0.7923 - val_loss: 0.1689 - val_contrastive_accuracy: 0.9854 - val_rank1_accuracy: 0.7542 - val_false_acceptance_rate: 0.0000e+00 - val_false_rejection_rate: 0.1014\n",
            "Epoch 8/10\n",
            "29/29 [==============================] - 11s 364ms/step - loss: 0.1426 - contrastive_accuracy: 0.9300 - rank1_accuracy: 0.3599 - false_acceptance_rate: 0.0000e+00 - false_rejection_rate: 0.5856 - val_loss: 0.1486 - val_contrastive_accuracy: 0.9896 - val_rank1_accuracy: 0.5458 - val_false_acceptance_rate: 0.0000e+00 - val_false_rejection_rate: 0.0725\n",
            "Epoch 9/10\n",
            "29/29 [==============================] - 11s 372ms/step - loss: 0.1345 - contrastive_accuracy: 0.9300 - rank1_accuracy: 0.2974 - false_acceptance_rate: 0.0000e+00 - false_rejection_rate: 0.5285 - val_loss: 0.1378 - val_contrastive_accuracy: 0.9896 - val_rank1_accuracy: 0.5688 - val_false_acceptance_rate: 0.0000e+00 - val_false_rejection_rate: 0.0588\n",
            "Epoch 10/10\n",
            "29/29 [==============================] - 13s 453ms/step - loss: 0.1582 - contrastive_accuracy: 0.9321 - rank1_accuracy: 0.4116 - false_acceptance_rate: 0.0000e+00 - false_rejection_rate: 0.5339 - val_loss: 0.1655 - val_contrastive_accuracy: 0.9896 - val_rank1_accuracy: 0.7146 - val_false_acceptance_rate: 0.0000e+00 - val_false_rejection_rate: 0.0735\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c3e20b07190>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_gen = SiameseGenerator(image_paths=train_paths, labels=train_labels, batch_size=32, dim=(72, 96))\n",
        "val_gen = SiameseGenerator(image_paths=val_paths, labels=val_labels, batch_size=32, dim=(72, 96))\n",
        "\n",
        "siamese_model.fit(train_gen, validation_data=val_gen, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHi4j1zevhr5",
        "outputId": "0a084739-053f-4493-822e-5795ea416141"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15/15 [==============================] - 4s 249ms/step - loss: 0.1629 - contrastive_accuracy: 0.9896 - rank1_accuracy: 0.7188 - false_acceptance_rate: 0.0000e+00 - false_rejection_rate: 0.0769\n",
            "Validation Loss: 0.1629384458065033, Contrastive Accuracy: 0.9895833134651184\n",
            "Rank-1 Accuracy: 0.71875, FAR: 0.0, FRR: 0.07692307978868484\n"
          ]
        }
      ],
      "source": [
        "results = siamese_model.evaluate(val_gen)\n",
        "print(f\"Validation Loss: {results[0]}, Contrastive Accuracy: {results[1]}\")\n",
        "print(f\"Rank-1 Accuracy: {results[2]}, FAR: {results[3]}, FRR: {results[4]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQB6HZsBClEa"
      },
      "outputs": [],
      "source": [
        "siamese_network = siamese_model.siamese_network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etrQMJhexr5L"
      },
      "outputs": [],
      "source": [
        "# siamese_model.suummary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZFTLsVkCpg-"
      },
      "outputs": [],
      "source": [
        "# Get the base model (which should contain the embedding layers)\n",
        "base_model = siamese_network.get_layer('model_2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmWJhrogCpeU"
      },
      "outputs": [],
      "source": [
        "# Find the embedding layer\n",
        "embedding_layer = None\n",
        "for layer in base_model.layers:\n",
        "    if 'dense' in layer.name and layer.output_shape[-1] == 128:\n",
        "        embedding_layer = layer\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2yOzrWEDROb",
        "outputId": "066a4304-cfb0-4076-8792-d94e8e46582b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)        [(None, 72, 96, 3)]          0         []                            \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)        [(None, 72, 96, 3)]          0         []                            \n",
            "                                                                                                  \n",
            " model_2 (Functional)        (None, 128)                  1488096   ['input_3[0][0]',             \n",
            "                                                          0          'input_4[0][0]']             \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)           (None, 1)                    0         ['model_2[0][0]',             \n",
            "                                                                     'model_2[1][0]']             \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 257)                  0         ['lambda_1[0][0]',            \n",
            "                                                                     'model_2[0][0]',             \n",
            "                                                                     'model_2[1][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 14880960 (56.77 MB)\n",
            "Trainable params: 14879936 (56.76 MB)\n",
            "Non-trainable params: 1024 (4.00 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "siamese_network.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsQA8cWQDXI1",
        "outputId": "0b901d5c-c7ff-4821-e561-72791acd2582"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " model_input (InputLayer)    [(None, 72, 96, 3)]       0         \n",
            "                                                                 \n",
            " model (Functional)          (None, 2, 3, 512)         14714688  \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 1, 1, 512)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 1, 1, 512)         2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " global_average_pooling2d_1  (None, 512)               0         \n",
            "  (GlobalAveragePooling2D)                                       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " embedding (Dense)           (None, 128)               32896     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14880960 (56.77 MB)\n",
            "Trainable params: 14879936 (56.76 MB)\n",
            "Non-trainable params: 1024 (4.00 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "base_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ClAiqoGXFm-",
        "outputId": "72084400-0d68-4ae5-ad4e-c2f86c653d16"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "embedding_model = Model(inputs=base_model.input, outputs=base_model.get_layer('embedding').output)\n",
        "embedding_model.save('/content/drive/MyDrive/WORKS/FACIAL RECOGNITION RESEARCH]/data/ExtractedTerravicDatabase_subset/embedding_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EO-k2aGlqTXj"
      },
      "outputs": [],
      "source": [
        "embedding_model.save('/content/drive/MyDrive/WORKS/FACIAL RECOGNITION RESEARCH]/data/ExtractedTerravicDatabase_subset/embedding_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qYMQ2GEw-Xd"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3Z1I1szxBb-"
      },
      "outputs": [],
      "source": [
        "def prepare_image(file_path, target_size=(72, 96)):\n",
        "    img = image.load_img(file_path, target_size=target_size)\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Convert single image to a batch.\n",
        "    img_array /= 255.0  # Normalize to [0,1]\n",
        "    return img_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXo3trJJwoXN"
      },
      "outputs": [],
      "source": [
        "def prepare_image(file_path, target_size=(72, 96)):\n",
        "    img = image.load_img(file_path, target_size=target_size)\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Convert single image to a batch.\n",
        "    img_array /= 255.0  # Normalize to [0,1]\n",
        "    return img_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quZeqNSnxMLV"
      },
      "outputs": [],
      "source": [
        "model_path = '/content/drive/MyDrive/WORKS/FACIAL RECOGNITION RESEARCH]/data/ExtractedTerravicDatabase_subset/embedding_model.keras'\n",
        "embedding_model.save(model_path)\n",
        "loaded_embedding_model = tf.keras.models.load_model(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hISvABiWxMIN",
        "outputId": "fb1c2464-3315-4aba-8cc1-f79738abf98e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n",
            "Extracted Embeddings: [[-0.01912479  0.40658548  0.18127044  0.00165754 -0.07165163 -0.15086706\n",
            "  -0.20860499 -0.07250863 -0.18301883 -0.1153632  -0.17011112  0.1999795\n",
            "   0.12043087  0.06304704 -0.4185797   0.2089615  -0.11215377  0.31171775\n",
            "  -0.30731562  0.16656616 -0.15939932 -0.04417293  0.10753877  0.18475549\n",
            "   0.24684365  0.04067276 -0.3096503   0.04711301  0.08653637 -0.05135997\n",
            "   0.40428606 -0.46968377 -0.4457499   0.11665329 -0.03427369 -0.29694575\n",
            "   0.00725576 -0.20936379 -0.09393168  0.02089271  0.05605834 -0.4077622\n",
            "  -0.02138385  0.09734756 -0.00691052  0.02385576  0.24062635  0.183377\n",
            "   0.21641287  0.3950888  -0.22205831 -0.09340579  0.20120788  0.07053374\n",
            "   0.05292066 -0.14440848  0.21112278  0.21500748  0.17875187  0.15930384\n",
            "  -0.2243227  -0.15412498 -0.2905882   0.41665027 -0.12045509 -0.05396507\n",
            "   0.08303372 -0.0051806   0.28940558 -0.4022919   0.00547352 -0.22404207\n",
            "   0.11083746 -0.29596812 -0.04672862 -0.02648696 -0.13263066 -0.06821787\n",
            "  -0.10470683 -0.14714219 -0.07909385  0.28554848 -0.14801116  0.29489434\n",
            "   0.1868658  -0.19394612  0.1606318   0.19370618 -0.33879903  0.01035262\n",
            "  -0.25113705 -0.24204558  0.0008049   0.22911376 -0.19776662 -0.2758465\n",
            "  -0.23281823 -0.11208383 -0.3115962   0.00231238  0.43164992  0.04985635\n",
            "   0.0746133  -0.18849023  0.00111482 -0.3437447   0.06971988 -0.11343066\n",
            "  -0.20843303 -0.09072587  0.09766036  0.31132296 -0.15799671 -0.03263099\n",
            "   0.45271012  0.10165533  0.12431394 -0.1101988  -0.349839   -0.28376183\n",
            "  -0.16917601 -0.06121607  0.21865192  0.06422487 -0.28140488  0.0141143\n",
            "  -0.11071933  0.1580699 ]]\n"
          ]
        }
      ],
      "source": [
        "test_image_path = '/content/drive/MyDrive/WORKS/FACIAL RECOGNITION RESEARCH]/data/ExtractedTerravicDatabase_subset/test/face01/0006.jpg'\n",
        "test_image = prepare_image(test_image_path)\n",
        "\n",
        "# Predict using the loaded model\n",
        "embeddings = loaded_embedding_model.predict(test_image)\n",
        "print(\"Extracted Embeddings:\", embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJ9ZInDRxMFZ",
        "outputId": "5126a185-c0df-42b8-fecf-10df4aaf5c4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n",
            "Extracted Embeddings: [[-0.02094031  0.40877208  0.18688153 -0.00347616 -0.07578442 -0.1451814\n",
            "  -0.21234547 -0.07569958 -0.18345675 -0.12039387 -0.1711103   0.19672127\n",
            "   0.11933121  0.06124888 -0.41469184  0.21259977 -0.10785224  0.31702247\n",
            "  -0.30645806  0.17044827 -0.16023329 -0.05319894  0.1201348   0.16867639\n",
            "   0.24136129  0.05263367 -0.30637202  0.04173392  0.08017248 -0.04092501\n",
            "   0.40267065 -0.4695762  -0.4435591   0.11615871 -0.04338807 -0.29666913\n",
            "  -0.00181285 -0.20613177 -0.09874157  0.01996687  0.05567691 -0.4147244\n",
            "  -0.02984027  0.10964737 -0.01350234  0.02307897  0.24514858  0.17680274\n",
            "   0.22250068  0.3954638  -0.21928732 -0.08507222  0.20889576  0.07213546\n",
            "   0.04706255 -0.1433383   0.22052243  0.23348609  0.16807412  0.16538654\n",
            "  -0.23381472 -0.14612229 -0.29268706  0.4197986  -0.12212327 -0.05907122\n",
            "   0.09595925 -0.00094076  0.2931495  -0.40101522  0.00955249 -0.22379045\n",
            "   0.10170807 -0.29331553 -0.04299953 -0.028485   -0.14776002 -0.07489059\n",
            "  -0.1126553  -0.14869288 -0.07331293  0.3049581  -0.14735323  0.2951939\n",
            "   0.18370019 -0.20435014  0.1607611   0.20797291 -0.34890997  0.00755469\n",
            "  -0.25796452 -0.23679325  0.00119466  0.22490737 -0.21175243 -0.28002772\n",
            "  -0.22396038 -0.1116963  -0.30335867 -0.0066045   0.4417869   0.05639489\n",
            "   0.06929048 -0.19508247 -0.00165366 -0.33889288  0.07317398 -0.11658735\n",
            "  -0.21861312 -0.07964067  0.09634032  0.30569792 -0.1635144  -0.04512389\n",
            "   0.45072073  0.08837592  0.13275701 -0.09261113 -0.3547794  -0.28215373\n",
            "  -0.18047681 -0.05101545  0.2191684   0.05359187 -0.28397143 -0.00296079\n",
            "  -0.12133438  0.16365692]]\n"
          ]
        }
      ],
      "source": [
        "comparison_image_path = '/content/drive/MyDrive/WORKS/FACIAL RECOGNITION RESEARCH]/data/ExtractedTerravicDatabase_subset/test/face01/0010.jpg'\n",
        "comparison_image = prepare_image(comparison_image_path)\n",
        "\n",
        "# Predict using the loaded model\n",
        "embeddings = loaded_embedding_model.predict(comparison_image)\n",
        "print(\"Extracted Embeddings:\", embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ou_cojReyIVo",
        "outputId": "0538c653-6930-4410-bc24-8dfbaebaa76c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Distance between embeddings: 0.08270331\n"
          ]
        }
      ],
      "source": [
        "# Get embeddings for both images\n",
        "embedding_1 = loaded_embedding_model.predict(test_image)\n",
        "embedding_2 = loaded_embedding_model.predict(comparison_image)\n",
        "\n",
        "# Calculate Euclidean distance as an example\n",
        "distance = np.linalg.norm(embedding_1 - embedding_2)\n",
        "print(\"Distance between embeddings:\", distance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_IaE6D8yIRS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dA7mruEdGadK"
      },
      "outputs": [],
      "source": [
        "base_model = siamese_network.get_layer('model_2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQDVCeCNQ7cz"
      },
      "outputs": [],
      "source": [
        "# # Now access the embedding layers from the base_model\n",
        "# embedding_model_1 = Model(inputs=base_model.input, outputs=base_model.get_layer('embedding').output)\n",
        "\n",
        "# # Save the model\n",
        "# embedding_model_1.save('/content/drive/MyDrive/WORKS/FACIAL RECOGNITION RESEARCH]/data/ExtractedTerravicDatabase_subset/embedding_model_1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMLRgIgj-ik3"
      },
      "outputs": [],
      "source": [
        "# Save the entire Siamese model\n",
        "siamese_model.save('/content/drive/MyDrive/WORKS/FACIAL RECOGNITION RESEARCH]/data/ExtractedTerravicDatabase_subset/siamese_fx_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ztu83SX5VdBW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
